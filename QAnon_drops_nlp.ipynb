{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Madness:\n",
    "## Analyzing linguistic and thematic patterns in QAnon 'drops'\n",
    "## An exercise in web scraping, dataframe creation and management, and natural language processing\n",
    "##### beautiful soup -> pandas dataframe\n",
    "##### request -> soup -> imgs and text -> pandas dataframe\n",
    "##### text -> cleaned -> tokenized -> remove stop words -> features dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as mpim\n",
    "import requests\n",
    "import itertools\n",
    "import re\n",
    "import string \n",
    "import nltk\n",
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import islice, zip_longest \n",
    "from skimage import io\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpParser, Tree\n",
    "from nltk.util import ngrams\n",
    "from urllib.request import Request, urlopen\n",
    "from IPython.display import Image\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/kylereaves/Documents/GitHub/parsing_madness'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "source": [
    "## Scraping and Parsing HTML with BeautifulSoup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://qposts.online'\n",
    "# since there are 104 pages total, an object to capture them all is created\n",
    "urls = ['https://qposts.online/page/{}'.format(i) for i in range(1, 105)]\n",
    "# a list comprehension requesting each url\n",
    "requests_urls = [requests.get(url) for url in urls]\n",
    "# a soup object for all of the requests \n",
    "soups = [BeautifulSoup(url.text, 'html.parser') for url in requests_urls]\n",
    "\n",
    "# the images and content of message will serve as the data for the dataframe\n",
    "all_images = [soup.findAll('img') for soup in soups]\n",
    "all_messages = [soup.findAll('div', 'message') for soup in soups]\n",
    "# the information contained in meta will serve as the dataframe's index\n",
    "all_meta = [soup.findAll('div', {'class': ['meta', 'lar']}) for soup in soups]\n",
    "\n",
    "punct = string.punctuation + str('’') + str('“') + str('”') + str('‘') + str('–') + str('…')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nums = []\n",
    "all_times = []\n",
    "for meta in all_meta:\n",
    "    for div in meta:\n",
    "        for span in div.findAll('span', {'class': ['num']}):\n",
    "            all_nums.append(span.text)\n",
    "        for span in div.findAll('span', {'class': ['time']}):\n",
    "            all_times.append(span.text)\n",
    "\n",
    "all_dates = pd.to_datetime(all_times, unit='s').date\n",
    "all_hours = pd.to_datetime(all_times, unit='s').time\n",
    "# some days Q posted multiple times; a multiindex is useful here\n",
    "all_multi = pd.MultiIndex.from_arrays([all_dates, all_hours], names=['date', 'hour'])\n",
    "\n",
    "\n",
    "# the website contains inconsistent tag use; the earliest posts use <br>'s\n",
    "# while the later posts use <p>; this needed to be accounted for with an if statement \n",
    "all_total = []\n",
    "for message in all_messages:\n",
    "    for msg in message:\n",
    "        inner = []\n",
    "        for br in msg.findAll('br'):\n",
    "            br.replace_with(' ')\n",
    "        for text in msg.findAll('div', class_='text'):\n",
    "            if text.next_element.name == 'p':\n",
    "                for p in text.findAll('p'):\n",
    "                    inner.append(p.get_text())\n",
    "            else:\n",
    "                inner.append(text.get_text())\n",
    "        for img in msg.findAll('img'):\n",
    "            inner.append(base_url+img['data-src'])\n",
    "        all_total.append([i for i in inner if i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_text = []\n",
    "for message in all_messages:\n",
    "    for msg in message:\n",
    "        inner = []\n",
    "        for br in msg.findAll('br'):\n",
    "            br.replace_with(' ')\n",
    "        for text in msg.findAll('div', class_='text'):\n",
    "            if text.next_element.name == 'p':\n",
    "                for p in text.findAll('p'):\n",
    "                    inner.append(p.get_text())\n",
    "        only_text.append([i for i in inner if i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[[\\]]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(punct), '', text)\n",
    "    text = re.sub(r'\\\\u2002', '', text)\n",
    "    text = re.sub(r'\\u2002', '', text)\n",
    "    text = re.sub(r'>>\\d+', '', text)\n",
    "    text = re.sub(r'\\w+.jp(e*)g|\\w+.png', '', text)\n",
    "    text = re.sub(r'\\t', '', text)\n",
    "    text = re.sub(r'…|_|!', '', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('drops.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(all_total, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "source": [
    "## pandas dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df = pd.DataFrame({'number': all_nums, 'q_drop': all_total}, index=all_multi)\n",
    "q_df['number'] = q_df['number'].astype('float')\n",
    "q_df['q_drop'] = q_df['q_drop'].astype('str')\n",
    "pd.set_option('display.max_colwidth', None)  \n",
    "\n",
    "def search(query):\n",
    "    query_df = q_df.loc[q_df.q_drop.str.contains(query)].copy()\n",
    "    return query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df.to_csv('q_drops.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_df = pd.read_csv('q_drops.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            date      hour  number  \\\n",
       "0     2020-12-08  22:05:50  4953.0   \n",
       "2     2020-11-13  03:20:17  4951.0   \n",
       "3     2020-11-13  02:32:39  4950.0   \n",
       "4     2020-11-03  06:27:36  4949.0   \n",
       "5     2020-11-02  22:48:50  4948.0   \n",
       "...          ...       ...     ...   \n",
       "4874  2017-11-05  04:16:50    79.0   \n",
       "4878  2017-11-05  04:14:37    75.0   \n",
       "4902  2017-11-02  18:12:06    51.0   \n",
       "4925  2017-11-01  05:59:01    28.0   \n",
       "4939  2017-11-01  03:00:15    14.0   \n",
       "\n",
       "                                                 q_drop  \n",
       "0     ['https://www.youtube.com/watch?v=O1l-nR1Apj4'...  \n",
       "2     ['Shall we play a game?', '[N]othing [C]an [S]...  \n",
       "3     ['Nothing can stop what is coming.', 'Nothing!...  \n",
       "4     ['https://www.youtube.com/watch?v=9tjdswqGGVg&...  \n",
       "5     ['https://twitter.com/TimMurtaugh/status/13233...  \n",
       "...                                                 ...  \n",
       "4874  ['Graphic is right. Add above points to graphi...  \n",
       "4878  ['By the time POTUS returns from his trip the ...  \n",
       "4902  ['>>147642589one-nation-under-god-t-shirt_desi...  \n",
       "4925  [\">>147450119Spy.png>>147441102 >What must be ...  \n",
       "4939  [\"SCI[F] Military Intelligence. What is 'State...  \n",
       "\n",
       "[2937 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>hour</th>\n      <th>number</th>\n      <th>q_drop</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-12-08</td>\n      <td>22:05:50</td>\n      <td>4953.0</td>\n      <td>['https://www.youtube.com/watch?v=O1l-nR1Apj4'...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-11-13</td>\n      <td>03:20:17</td>\n      <td>4951.0</td>\n      <td>['Shall we play a game?', '[N]othing [C]an [S]...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-11-13</td>\n      <td>02:32:39</td>\n      <td>4950.0</td>\n      <td>['Nothing can stop what is coming.', 'Nothing!...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-11-03</td>\n      <td>06:27:36</td>\n      <td>4949.0</td>\n      <td>['https://www.youtube.com/watch?v=9tjdswqGGVg&amp;...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2020-11-02</td>\n      <td>22:48:50</td>\n      <td>4948.0</td>\n      <td>['https://twitter.com/TimMurtaugh/status/13233...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4874</th>\n      <td>2017-11-05</td>\n      <td>04:16:50</td>\n      <td>79.0</td>\n      <td>['Graphic is right. Add above points to graphi...</td>\n    </tr>\n    <tr>\n      <th>4878</th>\n      <td>2017-11-05</td>\n      <td>04:14:37</td>\n      <td>75.0</td>\n      <td>['By the time POTUS returns from his trip the ...</td>\n    </tr>\n    <tr>\n      <th>4902</th>\n      <td>2017-11-02</td>\n      <td>18:12:06</td>\n      <td>51.0</td>\n      <td>['&gt;&gt;147642589one-nation-under-god-t-shirt_desi...</td>\n    </tr>\n    <tr>\n      <th>4925</th>\n      <td>2017-11-01</td>\n      <td>05:59:01</td>\n      <td>28.0</td>\n      <td>[\"&gt;&gt;147450119Spy.png&gt;&gt;147441102 &gt;What must be ...</td>\n    </tr>\n    <tr>\n      <th>4939</th>\n      <td>2017-11-01</td>\n      <td>03:00:15</td>\n      <td>14.0</td>\n      <td>[\"SCI[F] Military Intelligence. What is 'State...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2937 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "q_df[q_df.q_drop.str.contains('https')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_from_pickle = re.findall(r'https://qposts\\S+(?:jpg|jpeg|png)', str(drops_pickled))\n",
    "links_from_pickle = re.findall(r'\\bhttps?://(?!\\S+(?:jpe?g|png))\\S+', str(drops_pickled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('drops.pickle', 'rb') as f:\n",
    "    drops_pickled = pickle.load(f)"
   ]
  },
  {
   "source": [
    "### Instead of requesting and parsing soup objects each time, we can pickle outputs for optimization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(itertools.chain.from_iterable(total_pickled))"
   ]
  },
  {
   "source": [
    "### method for cleaning a pickled outpit"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_only_text = []\n",
    "for drop in drops_pickled:\n",
    "    for line in drop:\n",
    "        if line.startswith('https') or line.startswith('>>') or line == 'Q' or line.startswith('>'):\n",
    "            pass\n",
    "        else:\n",
    "            line = re.sub('[%s]' % re.escape(punct), '', line)\n",
    "            line = re.sub(r'https?\\w+', '', line)\n",
    "            line = re.sub(r'\\d(th)?', '', line)\n",
    "            line = re.sub(r'\\u2002', ' ', line)\n",
    "            line = re.sub(r'www\\w+', '', line)\n",
    "            line = line.lower()\n",
    "            if not line.strip():\n",
    "                pass\n",
    "            else:\n",
    "                pickled_only_text.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_text.pickle', 'wb') as f:\n",
    "    pickle.dump(pickled_only_text, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_text.pickle', 'rb') as f:\n",
    "    cleaned_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [nltk.word_tokenize(text) for text in cleaned_text]"
   ]
  },
  {
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "**steps for cleaning data**:\n",
    "* concact text into one string\n",
    "* make the text lowercase\n",
    "* remove punctuation \n",
    "* remove urls\n",
    "* remove numbers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bow(some_text):\n",
    "    bow_dictionary = {}\n",
    "    for text in some_text:\n",
    "        if text in bow_dictionary:\n",
    "            bow_dictionary[text] += 1\n",
    "        else:\n",
    "            bow_dictionary[text] = 1\n",
    "    return bow_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_cleaned_text = [text for text in cleaned_text if text and text != 'Q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped = [nltk.word_tokenize(tokens) for tokens in flat_cleaned_text if tokens not in stop_words]\n",
    "tagged = [nltk.pos_tag(stop) for stop in stopped]"
   ]
  },
  {
   "source": [
    "## Bag of Words dictionary "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_dict = text_to_bow(stopped_flat_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('q', 401),\n",
       " ('people', 389),\n",
       " ('news', 319),\n",
       " ('us', 300),\n",
       " ('think', 297),\n",
       " ('potus', 284),\n",
       " ('control', 283),\n",
       " ('public', 266),\n",
       " ('treason', 232),\n",
       " ('media', 226),\n",
       " ('fake', 209),\n",
       " ('one', 195),\n",
       " ('comey', 195),\n",
       " ('narrative', 185),\n",
       " ('2', 180),\n",
       " ('would', 180),\n",
       " ('brennan', 171),\n",
       " ('election', 169),\n",
       " ('1', 165),\n",
       " ('see', 163),\n",
       " ('time', 163),\n",
       " ('power', 159),\n",
       " ('truth', 152),\n",
       " ('party', 146),\n",
       " ('wwg1wga', 145),\n",
       " ('w', 144),\n",
       " ('political', 143),\n",
       " ('james', 143),\n",
       " ('fbi', 141),\n",
       " ('president', 141),\n",
       " ('china', 138),\n",
       " ('mueller', 134),\n",
       " ('new', 130),\n",
       " ('post', 130),\n",
       " ('john', 130),\n",
       " ('must', 129),\n",
       " ('ds', 129),\n",
       " ('know', 128),\n",
       " ('coming', 122),\n",
       " ('believe', 120),\n",
       " ('many', 119),\n",
       " ('happens', 116),\n",
       " ('attack', 114),\n",
       " ('united', 114),\n",
       " ('push', 112),\n",
       " ('anons', 112),\n",
       " ('state', 109),\n",
       " ('ca', 109),\n",
       " ('god', 108),\n",
       " ('last', 107)]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "Counter(stopped_flat_words).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_NPS(sentences):\n",
    "    patterns = \"\"\"\n",
    "        NP: {<JJ.*>+<NN.*>+}\n",
    "        \"\"\"\n",
    "    NPChunker = nltk.RegexpParser(patterns)\n",
    "    nps = []\n",
    "    for sent in sentences:\n",
    "        tree = NPChunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'NP':\n",
    "                t = subtree\n",
    "                t = ' '.join(word for word, tag in t.leaves())\n",
    "                nps.append(t)\n",
    "    return nps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_VPS(sentences):\n",
    "    patterns = \"\"\"VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}\"\"\"\n",
    "    VPChunker = nltk.RegexpParser(patterns)\n",
    "    vps = []\n",
    "    for sent in sentences:\n",
    "        tree = VPChunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'VP':\n",
    "                t = subtree\n",
    "                t = ' '.join(word for word, tag in t.leaves())\n",
    "                vps.append(t)\n",
    "    return vps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Enjoy the show', 28),\n",
       " ('are the news now', 18),\n",
       " ('was a time', 14),\n",
       " ('is power', 13),\n",
       " ('Watch the news', 12),\n",
       " ('is a reason', 11),\n",
       " ('is everything', 11),\n",
       " ('re a desired topic', 11),\n",
       " ('Ds push', 10),\n",
       " ('leading the attack', 10),\n",
       " ('expend ammunition', 9),\n",
       " ('organized use', 9),\n",
       " ('See something', 9),\n",
       " ('evinces a design', 8),\n",
       " ('challenge political control', 8),\n",
       " ('is a philosophical viewpoint', 8),\n",
       " ('regarding truth', 8),\n",
       " ('Follow the money', 8),\n",
       " ('make the connection', 8),\n",
       " ('pose no threat', 7),\n",
       " ('TELL the public', 7),\n",
       " ('regain power', 7),\n",
       " ('becomes a threat', 7),\n",
       " ('control the narrative andor', 7),\n",
       " ('have free thought', 7),\n",
       " ('is the only way forward', 7),\n",
       " ('think this war', 7),\n",
       " ('considered a major threat', 7),\n",
       " ('is a direct attack', 7),\n",
       " ('embedded link', 7),\n",
       " ('was clear verify', 7),\n",
       " ('launch a massive domestic foreign surv', 7),\n",
       " ('elect president', 7),\n",
       " ('verified report', 7),\n",
       " ('considering the funding', 7),\n",
       " ('was the opposition', 7),\n",
       " ('granted auth', 7),\n",
       " ('made aware', 7),\n",
       " ('was urgency', 7),\n",
       " ('given the gravity', 7),\n",
       " ('pursuing a crime', 7),\n",
       " ('expand the mandate not', 7),\n",
       " ('report that expansion', 7),\n",
       " ('seek a crime', 7),\n",
       " ('justify the appointment', 7),\n",
       " ('wasnt the mandate', 7),\n",
       " ('Was the purpose', 7),\n",
       " ('find a crime', 7),\n",
       " ('done everything', 6),\n",
       " ('is the word', 6)]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "Counter(list_of_VPS(tagged)).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Logical thinking', 49),\n",
       " ('GREAT AWAKENING', 18),\n",
       " ('only way', 15),\n",
       " ('full armor', 15),\n",
       " ('desired topic', 11),\n",
       " ('first time', 9),\n",
       " ('American people', 9),\n",
       " ('last night', 9),\n",
       " ('original mandate', 9),\n",
       " ('many people', 8),\n",
       " ('public exposure', 8),\n",
       " ('new Government', 8),\n",
       " ('such principles', 8),\n",
       " ('such form', 8),\n",
       " ('long train', 8),\n",
       " ('same Object', 8),\n",
       " ('such Government', 8),\n",
       " ('new Guards', 8),\n",
       " ('political control', 8),\n",
       " ('Democratic Former Mayor', 8),\n",
       " ('social media', 8),\n",
       " ('Free thought', 8),\n",
       " ('philosophical viewpoint', 8),\n",
       " ('logic reason', 8),\n",
       " ('foreign entity', 8),\n",
       " ('highest levels', 7),\n",
       " ('mathematical probability', 7),\n",
       " ('greatest fear', 7),\n",
       " ('transient causes', 7),\n",
       " ('old girl', 7),\n",
       " ('deep state', 7),\n",
       " ('nondogmatic ’ information', 7),\n",
       " ('narrative andor', 7),\n",
       " ('stable ‘ groupthink ’ collective', 7),\n",
       " ('free thought ’', 7),\n",
       " ('future events', 7),\n",
       " ('dark world', 7),\n",
       " ('spiritual forces', 7),\n",
       " ('devil ’ s schemes', 7),\n",
       " ('’ t', 7),\n",
       " ('public opinion', 7),\n",
       " ('illegal immigrants', 7),\n",
       " ('near future', 7),\n",
       " ('last year', 7),\n",
       " ('illegal SURV members', 7),\n",
       " ('major threat', 7),\n",
       " ('direct attack', 7),\n",
       " ('serious threats ability', 7),\n",
       " ('targeted accounts stages', 7),\n",
       " ('clear verify', 7)]"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "Counter(list_of_NPS(tagged)).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('only', 133),\n",
       " ('family', 50),\n",
       " ('really', 45),\n",
       " ('carefully', 40),\n",
       " ('simply', 38),\n",
       " ('knowingly', 33),\n",
       " ('directly', 28),\n",
       " ('daily', 27),\n",
       " ('early', 24),\n",
       " ('immediately', 20),\n",
       " ('mathematically', 20),\n",
       " ('actually', 19),\n",
       " ('publicly', 19),\n",
       " ('rally', 18),\n",
       " ('currently', 18),\n",
       " ('effectively', 17),\n",
       " ('july', 16),\n",
       " ('probably', 16),\n",
       " ('apply', 15),\n",
       " ('actively', 15)]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "Counter([word.lower() for word in re.findall('\\w+ly', str(tagged))]).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(('not', 'RB'), 382),\n",
       " (('now', 'RB'), 132),\n",
       " (('very', 'RB'), 91),\n",
       " (('just', 'RB'), 85),\n",
       " (('only', 'RB'), 85),\n",
       " (('here', 'RB'), 78),\n",
       " (('still', 'RB'), 70),\n",
       " (('so', 'RB'), 68),\n",
       " (('never', 'RB'), 58),\n",
       " (('always', 'RB'), 55),\n",
       " (('then', 'RB'), 55),\n",
       " (('prior', 'RB'), 55),\n",
       " (('again', 'RB'), 54),\n",
       " (('no', 'RB'), 53),\n",
       " (('back', 'RB'), 51),\n",
       " (('Sometimes', 'RB'), 50),\n",
       " (('really', 'RB'), 43),\n",
       " (('carefully', 'RB'), 39),\n",
       " (('ever', 'RB'), 39),\n",
       " (('too', 'RB'), 37),\n",
       " (('simply', 'RB'), 36),\n",
       " (('also', 'RB'), 34),\n",
       " (('already', 'RB'), 34),\n",
       " (('well', 'RB'), 33),\n",
       " (('long', 'RB'), 32),\n",
       " (('Now', 'RB'), 30),\n",
       " (('soon', 'RB'), 30),\n",
       " (('So', 'RB'), 29),\n",
       " (('longer', 'RB'), 28),\n",
       " (('directly', 'RB'), 27),\n",
       " (('even', 'RB'), 26),\n",
       " (('right', 'RB'), 25),\n",
       " (('far', 'RB'), 24),\n",
       " (('yet', 'RB'), 24),\n",
       " (('as', 'RB'), 24),\n",
       " (('there', 'RB'), 23),\n",
       " (('Not', 'RB'), 22),\n",
       " (('ago', 'RB'), 22),\n",
       " (('instead', 'RB'), 22),\n",
       " (('Only', 'RB'), 20),\n",
       " (('immediately', 'RB'), 20),\n",
       " (('forward', 'RB'), 19),\n",
       " (('actually', 'RB'), 19),\n",
       " (('away', 'RB'), 18),\n",
       " (('currently', 'RB'), 18),\n",
       " (('publicly', 'RB'), 18),\n",
       " (('enough', 'RB'), 18),\n",
       " (('Knowingly', 'RB'), 17),\n",
       " (('effectively', 'RB'), 17),\n",
       " (('Just', 'RB'), 16)]"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "Counter(filter(lambda x: (x[1] == 'RB'), flat_tagged)).most_common(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}