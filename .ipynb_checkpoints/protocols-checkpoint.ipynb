{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "proper-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "punct = string.punctuation + str('’') + str('“') + str('”') + str('‘') + str('–') + str('…')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "primary-ambassador",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kylereaves/Documents/GitHub/parsing_madness'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "provincial-blair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       {images.pickle',\n",
       " 'Untitled1.ipynb',\n",
       " 'Untitled.ipynb',\n",
       " 'image_downloader.ipynb',\n",
       " 'meta.pickle',\n",
       " 'q_drops.txt',\n",
       " 'QAnon_drops_nlp.ipynb',\n",
       " 'cleaned_text.pickle',\n",
       " '.ipynb_checkpoints',\n",
       " 'total_pickled.pkl',\n",
       " '.git',\n",
       " '01_ParsingMadness_BeauitfulSoup.ipynb',\n",
       " 'drops.pickle'],
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "hungarian-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols = '/Users/kylereaves/Desktop/conspiratorial_texts/protocols.txt'\n",
    "protocols_content = open(protocols, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "external-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\drd', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'PROTOCOL\\sNo.\\s\\d', '', text)\n",
    "    text = re.sub(r'\\d\\.?', '', text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "valued-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = clean_text(protocols_content)\n",
    "sents = sent_tokenize(cleaned.strip())\n",
    "words = [word_tokenize(sent) for sent in sents],
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "chinese-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = nltk.corpus.wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "  cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "  tokenized = word_tokenize(cleaned)\n",
    "  normalized = \" \".join([normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized if token not in stop_words])\n",
    "  return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "monthly-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = [preprocess_text(sent) for sent in sents]\n",
    "\n",
    "concacted_str = ' '.join(processed)\n",
    "\n",
    "counter = CountVectorizer()\n",
    "\n",
    "matrix = counter.fit_transform([concacted_str])\n",
    "\n",
    "features = counter.get_feature_names()\n",
    "\n",
    "protocols_df = pd.DataFrame(matrix.T.todense(), index=features, columns={Term Frequencies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "described-flour",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term Frequencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shall</th>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goy</th>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emancipation</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emasculate</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emblem</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>petard</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journalism</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2832 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Term Frequencies\n",
       "shall                      175\n",
       "state                      124\n",
       "goy                        108\n",
       "people                     106\n",
       "one                         77\n",
       "...                        ...\n",
       "emancipation                 1\n",
       "emasculate                   1\n",
       "emblem                       1\n",
       "petard                       1\n",
       "journalism                   1\n",
       "\n",
       [2832 rows x 1 columns],
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protocols_df.sort_values(by={Term Frequencies'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "centered-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(norm=None)\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed)\n",
    "\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "index = [line for line in processed]\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.T.todense(), index=tfidf_features, columns=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "opposite-cocktail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deduction       6.988961\n",
       "phrase          6.583496\n",
       "fine            6.295814\n",
       "significance    6.295814\n",
       "comparison      6.072671\n",
       "                  ...   \n",
       "exist           0.000000\n",
       "existence       0.000000\n",
       "expand          0.000000\n",
       "expect          0.000000\n",
       "zion            0.000000\n",
       "Name: put aside fine phrase shall speak significance think comparison deduction shall throw light upon surround fact, Length: 2832, dtype: float64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df[tfidf_df.columns[0]].sort_values(ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
